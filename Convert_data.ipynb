{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataset to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output file names\n",
    "- Change the variables below to match the location of data files of interest.\n",
    "- The cell below should also be where you set the working directory, if necessary\n",
    "- productInFile should be the metadata file and reviewInFile should be the k-cores file.  \n",
    "- productCsvName should be the wherever the products file should go (about 3Gb), reviewCsvName should be wherever the reviews go (about 24Gb), and peopleCsvName should be wherever the people file should go (about 90Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input files\n",
    "productInFile = \"data/metadata.json.gz\"\n",
    "reviewInFile = \"data/kcore_5.json.gz\"\n",
    "\n",
    "# output csv files\n",
    "productCsvName = \"data/products.csv\"\n",
    "reviewCsvName = \"data/reviews.csv\"\n",
    "peopleCsvName = \"data/people.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create product csv\n",
    "Everything here should be ready to go. If you want to test it out on a smaller set uncomment the line near the end with the \"break\" statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(productInFile, \"r\") as f, open(productCsvName, 'w') as csvProducts:\n",
    "    # create set for ensuring only unique items\n",
    "    prodSet = set()\n",
    "    # create csv writer\n",
    "    prod = csv.writer(csvProducts)\n",
    "    prod.writerow([\"asin\", \"name\", \"price\", \"imUrl\", \"brand\", \"categories\", \"rankCat\", \"rank\"])\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        ln = re.sub(\"\\n\", \"\", ln)\n",
    "        d = ast.literal_eval(ln)\n",
    "        if d.get(\"asin\") not in prodSet:\n",
    "            prodSet.add(d.get(\"asin\"))\n",
    "            tmpAs = d.get(\"asin\")\n",
    "            if tmpAs != None:\n",
    "                tmpAs = re.sub(\"\\n\", \" \", tmpAs)\n",
    "                tmpAs = tmpAs.replace(\"\\\\\", \"\")\n",
    "                tmpAs = tmpAs.replace(\",\", \"\")\n",
    "            sr = d.get(\"salesRank\")\n",
    "            if sr == None or len(sr) == 0:\n",
    "                sr = {\"NA\": 0}\n",
    "            sr2 = [list(sr.keys())[0], list(sr.values())[0]]\n",
    "            nm = d.get(\"tmp\")\n",
    "            if nm != None:\n",
    "                nm = re.sub(\"\\n\", \" \", nm)\n",
    "                nm = nm.replace(\"\\\\\", \"\")\n",
    "                nm = nm.replace(\",\", \"\")\n",
    "            ti = d.get(\"title\")\n",
    "            if ti != None:\n",
    "                ti = re.sub(\"\\n\", \" \", ti)\n",
    "                ti = ti.replace(\"\\\\\", \"\")\n",
    "                ti = ti.replace(\",\", \"\")\n",
    "                ti = ti.replace(\"\\\"\", \"\")\n",
    "                ti = ti.replace(\"\\'\", \"\")\n",
    "            prod.writerow([tmpAs, ti, d.get(\"price\"), d.get(\"imUrl\"),\n",
    "                          d.get(\"brand\"), d.get(\"categories\"),\n",
    "                          sr2[0], sr2[1]])\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "#         if count > 10000: break\n",
    "print(count, time.time()-sttime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create people and review csvs\n",
    "Everything here should be ready to go. If you want to test it out on a smaller set uncomment the line near the end with the \"break\" statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(reviewInFile, \"r\") as f, open(peopleCsvName, 'w') as csvPeople, open(reviewCsvName, 'w') as csvReviews:\n",
    "    # create set for ensuring only unique items\n",
    "    pplSet = set()\n",
    "    \n",
    "    # create csv writers\n",
    "    ppl = csv.writer(csvPeople)\n",
    "    ppl.writerow([\"reviewerID\", \"name\"])\n",
    "\n",
    "    rev = csv.writer(csvReviews)\n",
    "    rev.writerow([\"reviewerID\", \"score\", \"reviewText\", \"summary\", \"helpful0\",\n",
    "                  \"helpful1\", \"ts\", \"asin\"])\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        d = json.loads(ln)\n",
    "        \n",
    "        # add person\n",
    "        if d.get(\"reviewerID\") not in pplSet:\n",
    "            pplSet.add(d.get(\"reviewerID\"))\n",
    "            tempNm = d.get(\"reviewerName\")\n",
    "            if tempNm != None:\n",
    "                tempNm = re.sub(\"\\n\", \" \", tempNm)\n",
    "                tempNm = tempNm.replace(\"\\\\\", \"\")\n",
    "                tempNm = tempNm.replace(\",\", \"\")\n",
    "            ppl.writerow([d.get(\"reviewerID\"), tempNm])\n",
    "        # add review\n",
    "        tr = d.get(\"reviewText\")\n",
    "        tsu = d.get(\"summary\")\n",
    "        if tr != None:\n",
    "            tr = re.sub(\"\\n\", \" \", tr)\n",
    "            tr = tr.replace(\"\\\\\", \"\")\n",
    "            tr = tr.replace(\",\", \"\")\n",
    "        if tsu != None:\n",
    "            tsu = re.sub(\"\\n\", \" \", tsu)\n",
    "            tsu = tsu.replace(\"\\\\\", \"\")\n",
    "            tsu = tsu.replace(\",\", \"\")\n",
    "        rev.writerow([d.get(\"reviewerID\"), d.get(\"overall\"), tr, tsu,\n",
    "                      d.get(\"helpful\")[0], d.get(\"helpful\")[1],\n",
    "                      d.get(\"unixReviewTime\"), d.get(\"asin\")])\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "#         if count > 10000: break\n",
    "print(count, time.time()-sttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the data to a Dropbox folder\n",
    "The files are specified, you just need to change the location of the output folder (dropLocs - yes it's play on words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "dropLocs = \"C:/Users/danny/Dropbox/bigdata/\"\n",
    "shutil.copy(productCsvName, dropLocs)\n",
    "shutil.copy(reviewCsvName, dropLocs)\n",
    "shutil.copy(peopleCsvName, dropLocs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
