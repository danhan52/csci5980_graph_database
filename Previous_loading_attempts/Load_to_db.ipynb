{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset to server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a couple variables\n",
    "- doLocally determines where or not to run the server locally or at the location specified in the graphpass file\n",
    "- startFresh determines whether to delete all the nodes before running the code\n",
    "- fLoc determine where to get the files from (more details below)\n",
    "- there are three csv files and you can either use local files or dropbox files (I'll update later to hide the files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a couple of graph setup options\n",
    "doLocally = False\n",
    "startFresh = False\n",
    "\n",
    "# input files' location\n",
    "# fLoc = \"file:///Users/danny/Repos/csci5980_graph_database/data/\" # windows\n",
    "# fLoc = \"file:////home/danny/Repos/csci5980_graph_database/data/\" # linux\n",
    "# fLoc = \"file:///\" # with standard options\n",
    "fLoc = \"\" # doing it from dropbox\n",
    "\n",
    "# input csv files - local\n",
    "# productCsvName = \"products.csv\"\n",
    "# reviewCsvName = \"reviews.csv\"\n",
    "# peopleCsvName = \"people.csv\"\n",
    "# input csv files - dropbox\n",
    "# peopleCsvName = \"https://dl.dropbox.com/s/wepmkxqzt6xk0nk/people.csv\"\n",
    "# productCsvName = \"https://dl.dropbox.com/s/fv6b0vob2sbwlai/products.csv\"\n",
    "# reviewCsvName = \"https://dl.dropbox.com/s/orm7em01c27mmgi/reviews.csv\"\n",
    "peopleCsvName = \"https://dl.dropbox.com/s/8wnfq7c7ppkvxbc/people.csv\"\n",
    "productCsvName = \"https://dl.dropbox.com/s/scvk789n2xx0wcx/products.csv\"\n",
    "reviewCsvName = \"https://dl.dropbox.com/s/14eomoh7y229tjb/reviews.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries and start the graph connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://neo4j:AgentSmith@34.236.229.56:7474/db/data/\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from py2neo import Graph\n",
    "\n",
    "if doLocally:\n",
    "    server = \"http://neo4j:bigdata@localhost:7474/db/data/\"\n",
    "else:\n",
    "    with open(\"graphpass.txt\") as f:\n",
    "        server = f.read()\n",
    "print(server)\n",
    "graph = Graph(server)\n",
    "if startFresh:\n",
    "    graph.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the strings for the load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write query statements\n",
    "# statements asserting uniqueness\n",
    "q1 = \"\"\"\n",
    "create constraint on (pe:Person) assert pe.id is unique;\n",
    "\"\"\"\n",
    "q2 = \"\"\"\n",
    "create constraint on (pr:Product) assert pr.id is unique;\n",
    "\"\"\"\n",
    "\n",
    "# load reviews\n",
    "fnRev = fLoc + reviewCsvName\n",
    "qRev = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "merge (person:Person {id:row.reviewerID})\n",
    "merge (product:Product {id:row.asin})\n",
    "create (person)-[:Reviewed {ts:row.ts, reviewText:row.reviewText, score:row.score, summary:row.summary, helpful0:row.helpful0, helpful1:row.helpful1}]->(product);\n",
    "\"\"\" % fnRev\n",
    "\n",
    "# load people\n",
    "fnPpl = fLoc + peopleCsvName\n",
    "qPpl = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(person:Person {id:row.reviewerID})\n",
    "set person.name = row.name;\n",
    "\"\"\" % fnPpl\n",
    "\n",
    "# load products\n",
    "fnProd = fLoc + productCsvName\n",
    "qProd = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(product:Product {id:row.asin})\n",
    "set product.name = row.name, product.price = row.price, product.imUrl = row.imUrl,\n",
    "    product.brand = row.brand, product.rankCat = row.rankCat, product.rank = row.rank,\n",
    "    product.categories = row.categories;\n",
    "\"\"\" % fnProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the queries\n",
    "If using the whole k-cores dataset (40 million reviews, 9 million products), it should take roughly 3 hours. Or at least, it took that long on my i5-processor desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('reviews', 42.6697051525116)\n",
      "('people', 6.222497940063477)\n",
      "('products', 4.918737888336182)\n"
     ]
    }
   ],
   "source": [
    "# run all the queries\n",
    "# run constraints\n",
    "graph.run(q1)\n",
    "graph.run(q2)\n",
    "\n",
    "# from py2neo import watch\n",
    "# watch(\"neo4j.bolt\")\n",
    "# run reviews first - it creates all the nodes\n",
    "inittime = time.time()\n",
    "sttime = time.time()\n",
    "graph.run(qRev)\n",
    "print(\"reviews\", time.time()-sttime)\n",
    "with open(\"/home/danny/Dropbox/bigdata/revdone.txt\", \"w\") as f:\n",
    "    f.write(str(time.time()-sttime))\n",
    "\n",
    "\n",
    "# add names to people\n",
    "sttime = time.time()\n",
    "graph.run(qPpl)\n",
    "print(\"people\", time.time()-sttime)\n",
    "with open(\"/home/danny/Dropbox/bigdata/ppldone.txt\", \"w\") as f:\n",
    "    f.write(str(time.time()-sttime))\n",
    "\n",
    "# add misc information to products\n",
    "sttime = time.time()\n",
    "graph.run(qProd)\n",
    "print(\"products\", time.time()-sttime)\n",
    "finalleng = time.time() - inittime\n",
    "with open(\"/home/danny/Dropbox/bigdata/proddone.txt\", \"w\") as f:\n",
    "    f.write(str(time.time()-sttime))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(finalleng))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
