{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import shutil\n",
    "import os\n",
    "import dropbox\n",
    "from py2neo import Graph\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input files\n",
    "prodInFile = \"data/metadata.json.gz\"\n",
    "revInFile = \"data/kcore_5.json.gz\"\n",
    "\n",
    "# output csv files\n",
    "prodCsvName = \"data/products.csv\"\n",
    "revCsvName = \"data/reviews.csv\"\n",
    "pplCsvName = \"data/people.csv\"\n",
    "\n",
    "# dropbox output folder\n",
    "dropLocs = \"/home/danny/Dropbox/Apps/bigdataclass/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"dboxkey.txt\") as f:\n",
    "        key = f.read()\n",
    "dbx = dropbox.Dropbox(key)\n",
    "# dbx.users_get_current_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the graph connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://neo4j:AgentSmith@34.195.7.211:7474/db/data/\n"
     ]
    }
   ],
   "source": [
    "# a couple of graph setup options\n",
    "doLocally = False\n",
    "startFresh = False\n",
    "\n",
    "if doLocally:\n",
    "    server = \"http://neo4j:bigdata@localhost:7474/db/data/\"\n",
    "else:\n",
    "    with open(\"graphpass.txt\") as f:\n",
    "        server = f.read()\n",
    "print(server)\n",
    "graph = Graph(server)\n",
    "if startFresh:\n",
    "    graph.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x7f8f5c2dad50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statements asserting uniqueness\n",
    "q1 = \"\"\"\n",
    "create constraint on (pe:Person) assert pe.id is unique;\n",
    "\"\"\"\n",
    "q2 = \"\"\"\n",
    "create constraint on (pr:Product) assert pr.id is unique;\n",
    "\"\"\"\n",
    "graph.run(q1)\n",
    "graph.run(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open gzip json and write\n",
    "def makeRevFile(revInFile, revCsvName, startLn, endLn):\n",
    "    sttime = time.time() # time the process\n",
    "    with gzip.open(revInFile, \"r\") as f, open(revCsvName, 'w') as csvRev:\n",
    "        # write first row of csv\n",
    "        rev = csv.writer(csvRev)\n",
    "        rev.writerow([\"reviewerID\", \"score\", \"reviewText\", \"summary\", \"helpful0\",\n",
    "                      \"helpful1\", \"ts\", \"asin\"])\n",
    "        \n",
    "        count = 0\n",
    "        for line in islice(f, startLn, endLn):\n",
    "            ln = line.decode(\"ascii\")\n",
    "            d = json.loads(ln)\n",
    "\n",
    "            # add review\n",
    "            tr = d.get(\"reviewText\")\n",
    "            tsu = d.get(\"summary\")\n",
    "            if tr != None:\n",
    "                tr = re.sub(\"\\n\", \" \", tr)\n",
    "                tr = tr.replace(\"\\\\\", \"\")\n",
    "                tr = tr.replace(\",\", \"\")\n",
    "            if tsu != None:\n",
    "                tsu = re.sub(\"\\n\", \" \", tsu)\n",
    "                tsu = tsu.replace(\"\\\\\", \"\")\n",
    "                tsu = tsu.replace(\",\", \"\")\n",
    "            rev.writerow([d.get(\"reviewerID\"), d.get(\"overall\"), tr, tsu,\n",
    "                          d.get(\"helpful\")[0], d.get(\"helpful\")[1],\n",
    "                          d.get(\"unixReviewTime\"), d.get(\"asin\")])\n",
    "            count += 1\n",
    "    print(count, time.time()-sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load reviews\n",
    "qRev = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "merge (person:Person {id:row.reviewerID})\n",
    "merge (product:Product {id:row.asin})\n",
    "create (person)-[:Reviewed {ts:row.ts, reviewText:row.reviewText, score:row.score, summary:row.summary, helpful0:row.helpful0, helpful1:row.helpful1}]->(product);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # get the number of reviews\n",
    "# sttime = time.time() # time the process\n",
    "# numLines = sum(1 for line in gzip.open(revInFile))\n",
    "# print(numLines, time.time()-sttime)\n",
    "numLines = 41135700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose the chunks to upload\n",
    "# restart db every time you finish a chunk\n",
    "itt = list(range(0, numLines, 1000000))\n",
    "itt.append(numLines)\n",
    "itt = itt[32:]\n",
    "print(itt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# upload the section of the database in \"itt\"\n",
    "for i in range(len(itt)-1):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"start: \", itt[i])\n",
    "    # create the csv for upload\n",
    "    makeRevFile(revInFile, revCsvName, itt[i], itt[i+1])\n",
    "    # copy the csv to the dropbox folder\n",
    "    shutil.copy(revCsvName, dropLocs)\n",
    "    # wait until the file uploads\n",
    "    resp = dbx.files_list_folder(\"/data\")\n",
    "    while len(resp.entries) == 0:\n",
    "        print \".\",\n",
    "        time.sleep(5)\n",
    "        resp = dbx.files_list_folder(\"/data\")\n",
    "    print(\"On Dropbox\")\n",
    "    # get the dropbox url for the file\n",
    "    for fil in resp.entries:\n",
    "        fl = fil.path_lower\n",
    "    link = dbx.sharing_create_shared_link(fl)\n",
    "    url = link.url\n",
    "    url = url.replace(\"?dl=0\", \"\")\n",
    "    url = url.replace(\"https://www.\", \"https://dl.\")\n",
    "    # upload to the database\n",
    "    sttime = time.time() # time the process\n",
    "    graph.run(qRev % url)\n",
    "    print(\"upload\", time.time()-sttime)\n",
    "    # delete the file to start again\n",
    "    dbx.files_delete(\"/\" + revCsvName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload people data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(revInFile, \"r\") as f, open(pplCsvName, 'w') as csvPpl:\n",
    "    # create set for ensuring only unique items\n",
    "    pplSet = set()\n",
    "    \n",
    "    # create csv writers\n",
    "    ppl = csv.writer(csvPpl)\n",
    "    ppl.writerow([\"reviewerID\", \"name\"])\n",
    "\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        d = json.loads(ln)\n",
    "        \n",
    "        # add person\n",
    "        if d.get(\"reviewerID\") not in pplSet:\n",
    "            pplSet.add(d.get(\"reviewerID\"))\n",
    "            tempNm = d.get(\"reviewerName\")\n",
    "            if tempNm != None:\n",
    "                tempNm = re.sub(\"\\n\", \" \", tempNm)\n",
    "                tempNm = tempNm.replace(\"\\\\\", \"\")\n",
    "                tempNm = tempNm.replace(\",\", \"\")\n",
    "            ppl.writerow([d.get(\"reviewerID\"), tempNm])\n",
    "        count += 1\n",
    "        if count % 5000000 == 0:\n",
    "            print(count)\n",
    "print(count, time.time()-sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy the csv to the dropbox folder\n",
    "shutil.copy(pplCsvName, dropLocs)\n",
    "# wait until the file uploads\n",
    "resp = dbx.files_list_folder(\"/data\")\n",
    "while len(resp.entries) == 0:\n",
    "    print \".\",\n",
    "    time.sleep(5)\n",
    "    resp = dbx.files_list_folder(\"/data\")\n",
    "print(\"On Dropbox\")\n",
    "# get the dropbox url for the file\n",
    "for fil in resp.entries:\n",
    "    fl = fil.path_lower\n",
    "link = dbx.sharing_create_shared_link(fl)\n",
    "url = link.url\n",
    "url = url.replace(\"?dl=0\", \"\")\n",
    "url = url.replace(\"https://www.\", \"https://dl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load people\n",
    "qPpl = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(person:Person {id:row.reviewerID})\n",
    "set person.name = row.name;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# upload to the database\n",
    "sttime = time.time() # time the process\n",
    "graph.run(qPpl % url)\n",
    "print(\"upload\", time.time()-sttime)\n",
    "# delete the file to start again\n",
    "dbx.files_delete(\"/\" + pplCsvName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create product csv\n",
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(prodInFile, \"r\") as f, open(prodCsvName, 'w') as csvProd:\n",
    "    # create set for ensuring only unique items\n",
    "    prodSet = set()\n",
    "    # create csv writer\n",
    "    prod = csv.writer(csvProd)\n",
    "    prod.writerow([\"asin\", \"name\", \"price\", \"imUrl\", \"brand\", \"categories\", \"rankCat\", \"rank\"])\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        ln = re.sub(\"\\n\", \"\", ln)\n",
    "        d = ast.literal_eval(ln)\n",
    "        if d.get(\"asin\") not in prodSet:\n",
    "            prodSet.add(d.get(\"asin\"))\n",
    "            tmpAs = d.get(\"asin\")\n",
    "            if tmpAs != None:\n",
    "                tmpAs = re.sub(\"\\n\", \" \", tmpAs)\n",
    "                tmpAs = tmpAs.replace(\"\\\\\", \"\")\n",
    "                tmpAs = tmpAs.replace(\",\", \"\")\n",
    "            sr = d.get(\"salesRank\")\n",
    "            if sr == None or len(sr) == 0:\n",
    "                sr = {\"NA\": 0}\n",
    "            sr2 = [list(sr.keys())[0], list(sr.values())[0]]\n",
    "            nm = d.get(\"tmp\")\n",
    "            if nm != None:\n",
    "                nm = re.sub(\"\\n\", \" \", nm)\n",
    "                nm = nm.replace(\"\\\\\", \"\")\n",
    "                nm = nm.replace(\",\", \"\")\n",
    "            ti = d.get(\"title\")\n",
    "            if ti != None:\n",
    "                ti = re.sub(\"\\n\", \" \", ti)\n",
    "                ti = ti.replace(\"\\\\\", \"\")\n",
    "                ti = ti.replace(\",\", \"\")\n",
    "                ti = ti.replace(\"\\\"\", \"\")\n",
    "                ti = ti.replace(\"\\'\", \"\")\n",
    "            prod.writerow([tmpAs, ti, d.get(\"price\"), d.get(\"imUrl\"),\n",
    "                          d.get(\"brand\"), d.get(\"categories\"),\n",
    "                          sr2[0], sr2[1]])\n",
    "        count += 1\n",
    "        if count % 1000000 == 0:\n",
    "            print(count)\n",
    "print(count, time.time()-sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy the csv to the dropbox folder\n",
    "shutil.copy(prodCsvName, dropLocs)\n",
    "# wait until the file uploads\n",
    "resp = dbx.files_list_folder(\"/data\")\n",
    "while len(resp.entries) == 0:\n",
    "    print \".\",\n",
    "    time.sleep(5)\n",
    "    resp = dbx.files_list_folder(\"/data\")\n",
    "print(\"On Dropbox\")\n",
    "# get the dropbox url for the file\n",
    "for fil in resp.entries:\n",
    "    fl = fil.path_lower\n",
    "link = dbx.sharing_create_shared_link(fl)\n",
    "url = link.url\n",
    "url = url.replace(\"?dl=0\", \"\")\n",
    "url = url.replace(\"https://www.\", \"https://dl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load products\n",
    "qProd = \"\"\"\n",
    "using periodic commit 1000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(product:Product {id:row.asin})\n",
    "set product.name = row.name, product.price = row.price, product.imUrl = row.imUrl,\n",
    "    product.brand = row.brand, product.rankCat = row.rankCat, product.rank = row.rank,\n",
    "    product.categories = row.categories;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upload to the database\n",
    "sttime = time.time() # time the process\n",
    "graph.run(qProd % url)\n",
    "print(\"upload\", time.time()-sttime)\n",
    "# delete the file to start again\n",
    "dbx.files_delete(\"/\" + prodCsvName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate records\n",
    "Doesn't work on full data, too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# qDedup = \"\"\"\n",
    "# MATCH (pe)-[r]->(pr)\n",
    "# WITH pe, pr, TAIL (COLLECT (r)) as rr limit 1000\n",
    "# FOREACH (r IN rr | DELETE r)\n",
    "# \"\"\"\n",
    "\n",
    "# sttime = time.time() # time the process\n",
    "# graph.run(qDedup)\n",
    "# print(\"deduplicate\", time.time()-sttime)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
