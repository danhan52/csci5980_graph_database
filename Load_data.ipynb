{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create product csv\n",
    "fname = \"data/metadata.json.gz\"\n",
    "csvProducts = open('data/products.csv', 'w')\n",
    "prod = csv.writer(csvProducts)\n",
    "prod.writerow([\"asin\", \"name\", \"price\", \"imUrl\", \"brand\", \"categories\", \"rankCat\", \"rank\"])\n",
    "prodSet = set()\n",
    "\n",
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(fname, \"r\") as f:\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        ln = re.sub(\"\\n\", \"\", ln)\n",
    "        d = ast.literal_eval(ln)\n",
    "        if d.get(\"asin\") not in prodSet:\n",
    "            prodSet.add(d.get(\"asin\"))\n",
    "            tmpAs = d.get(\"asin\")\n",
    "            if tmpAs != None:\n",
    "                tmpAs = re.sub(\"\\n\", \" \", tmpAs)\n",
    "                tmpAs = tmpAs.replace(\"\\\\\", \"\")\n",
    "                tmpAs = tmpAs.replace(\",\", \"\")\n",
    "            sr = d.get(\"salesRank\")\n",
    "            if sr == None or len(sr) == 0:\n",
    "                sr = {\"NA\": 0}\n",
    "            sr2 = [list(sr.keys())[0], list(sr.values())[0]]\n",
    "            nm = d.get(\"tmp\")\n",
    "            if nm != None:\n",
    "                nm = re.sub(\"\\n\", \" \", nm)\n",
    "                nm = nm.replace(\"\\\\\", \"\")\n",
    "                nm = nm.replace(\",\", \"\")\n",
    "            ti = d.get(\"title\")\n",
    "            if ti != None:\n",
    "                ti = re.sub(\"\\n\", \" \", ti)\n",
    "                ti = ti.replace(\"\\\\\", \"\")\n",
    "                ti = ti.replace(\",\", \"\")\n",
    "                ti = ti.replace(\"\\\"\", \"\")\n",
    "                ti = ti.replace(\"\\'\", \"\")\n",
    "            prod.writerow([tmpAs, ti, d.get(\"price\"), d.get(\"imUrl\"),\n",
    "                          d.get(\"brand\"), d.get(\"categories\"),\n",
    "                          sr2[0], sr2[1]])\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "            prod.flush()\n",
    "#         if count > 10000: break\n",
    "csvProducts.close()\n",
    "print(count, time.time()-sttime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose which file to use\n",
    "fname = \"data/kcore_5.json.gz\"\n",
    "\n",
    "# create csv writers\n",
    "csvPeople = open('data/people.csv', 'w')\n",
    "ppl = csv.writer(csvPeople)\n",
    "ppl.writerow([\"reviewerID\", \"name\"])\n",
    "\n",
    "csvReviews = open('data/reviews.csv', 'w')\n",
    "rev = csv.writer(csvReviews)\n",
    "rev.writerow([\"reviewerID\", \"score\", \"reviewText\", \"summary\", \"helpful0\",\n",
    "              \"helpful1\", \"ts\", \"asin\"])\n",
    "\n",
    "# create sets for ensuring only unique items\n",
    "pplSet = set()\n",
    "\n",
    "# open gzip json and write\n",
    "sttime = time.time() # time the process\n",
    "count = 0\n",
    "with gzip.open(fname, \"r\") as f:\n",
    "    for line in f:\n",
    "        ln = line.decode(\"ascii\")\n",
    "        d = json.loads(ln)\n",
    "        \n",
    "        # add person\n",
    "        if d.get(\"reviewerID\") not in pplSet:\n",
    "            pplSet.add(d.get(\"reviewerID\"))\n",
    "            tempNm = d.get(\"reviewerName\")\n",
    "            if tempNm != None:\n",
    "                tempNm = re.sub(\"\\n\", \" \", tempNm)\n",
    "                tempNm = tempNm.replace(\"\\\\\", \"\")\n",
    "                tempNm = tempNm.replace(\",\", \"\")\n",
    "            ppl.writerow([d.get(\"reviewerID\"), tempNm])\n",
    "        # add review\n",
    "        tr = d.get(\"reviewText\")\n",
    "        tsu = d.get(\"summary\")\n",
    "        if tr != None:\n",
    "            tr = re.sub(\"\\n\", \" \", tr)\n",
    "            tr = tr.replace(\"\\\\\", \"\")\n",
    "            tr = tr.replace(\",\", \"\")\n",
    "        if tsu != None:\n",
    "            tsu = re.sub(\"\\n\", \" \", tsu)\n",
    "            tsu = tsu.replace(\"\\\\\", \"\")\n",
    "            tsu = tsu.replace(\",\", \"\")\n",
    "        rev.writerow([d.get(\"reviewerID\"), d.get(\"overall\"), tr, tsu,\n",
    "                      d.get(\"helpful\")[0], d.get(\"helpful\")[1],\n",
    "                      d.get(\"unixReviewTime\"), d.get(\"asin\")])\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            print(count)\n",
    "            rev.flush()\n",
    "            ppl.flush()\n",
    "#         if count > 10000: break\n",
    "print(count, time.time()-sttime)\n",
    "\n",
    "csvPeople.close()\n",
    "csvReviews.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy(\"data/people.csv\", \"C:/Users/danny/Dropbox/bigdata/\")\n",
    "shutil.copy(\"data/products.csv\", \"C:/Users/danny/Dropbox/bigdata/\")\n",
    "shutil.copy(\"data/reviews.csv\", \"C:/Users/danny/Dropbox/bigdata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"http://neo4j:bigdata@localhost:7474/db/data/\")\n",
    "# graph = Graph(\"http://neo4j:AgentSmith@34.236.229.56:7474/db/data/\")\n",
    "# @ whatever ip address amazon is on\n",
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write query statements\n",
    "# fLoc = \"file:///Users/danny/Repos/csci5980_graph_database/data/\"\n",
    "fLoc = \"file:///\"\n",
    "# statements asserting uniqueness\n",
    "q1 = \"\"\"\n",
    "create constraint on (pe:Person) assert pe.id is unique;\n",
    "\"\"\"\n",
    "q2 = \"\"\"\n",
    "create constraint on (pr:Product) assert pr.id is unique;\n",
    "\"\"\"\n",
    "\n",
    "# load people\n",
    "# fnPpl = fLoc + \"people.csv\"\n",
    "fnPpl = \"https://dl.dropbox.com/s/8wnfq7c7ppkvxbc/people.csv\"\n",
    "qPpl = \"\"\"\n",
    "using periodic commit 10000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(person:Person {id:row.reviewerID})\n",
    "set person.name = row.name;\n",
    "\"\"\" % fnPpl\n",
    "\n",
    "# load products\n",
    "# fnProd = fLoc + \"products.csv\"\n",
    "fnProd = \"https://dl.dropbox.com/s/scvk789n2xx0wcx/products.csv\"\n",
    "qProd = \"\"\"\n",
    "using periodic commit 10000\n",
    "load csv with headers from \"%s\" as row\n",
    "match(product:Product {id:row.asin})\n",
    "set product.name = row.name, product.price = row.price, product.imUrl = row.imUrl,\n",
    "    product.brand = row.brand, product.rankCat = row.rankCat, product.rank = row.rank,\n",
    "    product.categories = row.categories;\n",
    "\"\"\" % fnProd\n",
    "\n",
    "# load reviews\n",
    "# fnRev = fLoc + \"reviews.csv\"\n",
    "fnRev = \"https://dl.dropbox.com/s/14eomoh7y229tjb/reviews.csv\"\n",
    "qRev = \"\"\"\n",
    "using periodic commit 10000\n",
    "load csv with headers from \"%s\" as row\n",
    "with row limit 10000000\n",
    "merge (person:Person {id:row.reviewerID})\n",
    "merge (product:Product {id:row.asin})\n",
    "create (person)-[:Reviewed {ts:row.unixReviewTime, reviewText:row.reviewText, score:row.overall, summary:row.summary}]->(product);\n",
    "\"\"\" % fnRev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all the queries\n",
    "sttime = time.time()\n",
    "graph.run(q1)\n",
    "graph.run(q2)\n",
    "\n",
    "graph.run(qRev)\n",
    "print(\"reviews\", time.time()-sttime)\n",
    "graph.run(qPpl)\n",
    "print(\"people\", time.time()-sttime)\n",
    "graph.run(qProd)\n",
    "print(\"products\", time.time()-sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/products_old.csv\", \"r\") as f:\n",
    "#     re = csv.reader(f)\n",
    "#     count = 0\n",
    "#     for line in re:\n",
    "#         count += 1\n",
    "# #         print(line)\n",
    "# #         if count > 10: break\n",
    "#         if \"LG C\" in line:\n",
    "#             print(line)\n",
    "# #             break\n",
    "# print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
